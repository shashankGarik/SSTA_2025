{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f86a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149436e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarikipati7/.env/ssta_env/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "x = torch.rand(1,3,128,128)\n",
    "model = KeypointPredictor()\n",
    "out, soft, m = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079d8536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,64,32,32)\n",
    "y = torch.rand(1,100,2)\n",
    "\n",
    "model = ImageDecoder(64, 100, 3)\n",
    "out = model(x,y)\n",
    "out.shape\n",
    "\n",
    "model = ImageEncoder()\n",
    "out = model(torch.rand(1,3,128,128))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e03aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "class AlphaScheduler(Callback):\n",
    "    def __init__(self, warmup_epochs=10, ramp_epochs=20, final_alpha=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            warmup_epochs: Number of epochs to keep alpha = 0\n",
    "            ramp_epochs: Number of epochs to linearly ramp up alpha after warmup\n",
    "            final_alpha: Target alpha value\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.ramp_epochs = ramp_epochs\n",
    "        self.final_alpha = final_alpha\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "\n",
    "        if epoch < self.warmup_epochs:\n",
    "            alpha = 0.0\n",
    "        elif epoch < self.warmup_epochs + self.ramp_epochs:\n",
    "            # Linear ramp from 0 to final_alpha\n",
    "            ramp_progress = (epoch - self.warmup_epochs) / self.ramp_epochs\n",
    "            alpha = ramp_progress * self.final_alpha\n",
    "        else:\n",
    "            alpha = self.final_alpha\n",
    "\n",
    "        pl_module.alpha = alpha\n",
    "\n",
    "def draw_keypoints_on_image(image_tensor, keypoints, color='r'):\n",
    "    \"\"\"\n",
    "    image_tensor: [3, H, W] (float in [0,1])\n",
    "    keypoints: [N, 2] with (x, y) format in image coordinates\n",
    "    \"\"\"\n",
    "    image = TF.to_pil_image(image_tensor.cpu())\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    for x, y in keypoints.cpu():\n",
    "        plt.scatter(x, y, c=color, s=10)\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='jpeg', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    pil_image = Image.open(buf)\n",
    "    return TF.to_tensor(pil_image)\n",
    "    \n",
    "class LitKeypointDetector(L.LightningModule):\n",
    "    def __init__(self, keypoint_encoder, feature_encoder, feature_decoder):\n",
    "        super().__init__()\n",
    "        self.keypoint_generator = keypoint_encoder\n",
    "        self.feature_encoder = feature_encoder\n",
    "        self.image_reconstructor = feature_decoder\n",
    "        self.alpha = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        v1, vt = batch\n",
    "        \n",
    "        v1_features = self.feature_encoder(v1)\n",
    "\n",
    "        _, _, v1_heatmaps = self.keypoint_generator(v1)\n",
    "        _, vt_soft_keypoints, vt_heatmaps = self.keypoint_generator(vt)\n",
    "\n",
    "        vt_pred = self.image_reconstructor(v1_features, vt_soft_keypoints)\n",
    "        loss, mse, condens = self.keypoint_loss(vt, vt_pred, v1_heatmaps, vt_heatmaps)\n",
    "        self.log(\"train/total_loss\", loss)\n",
    "        self.log(\"train/mse\", mse)\n",
    "        self.log(\"train/condensation_loss\", condens)\n",
    "        self.log(\"alpha\", self.alpha)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        v1, vt = batch\n",
    "\n",
    "        # Encode features from v0\n",
    "        v1_features = self.feature_encoder(v1)\n",
    "\n",
    "        # Get keypoints from v0 and v1 separately\n",
    "        _, soft_kp_vt, v1_heatmaps = self.keypoint_generator(v1)\n",
    "        _, soft_kp_vt, vt_heatmaps = self.keypoint_generator(vt)\n",
    "\n",
    "        # Predict v1 using v0 features + v1 keypoints\n",
    "        vt_pred = self.image_reconstructor(v1_features, soft_kp_vt)\n",
    "        loss, mse, condens = self.keypoint_loss(vt, vt_pred, v1_heatmaps, vt_heatmaps)\n",
    "\n",
    "        self.log(\"val/total_loss\", loss)\n",
    "        self.log(\"val/mse\", mse)\n",
    "        self.log(\"val/condensation_loss\", condens)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            B, C, img_H, img_W = v1.shape\n",
    "            _, _, heatmap_H, heatmap_W = vt_heatmaps.shape\n",
    "\n",
    "            scale_x = img_W / heatmap_W\n",
    "            scale_y = img_H / heatmap_H\n",
    "\n",
    "            # Rescale keypoints\n",
    "            scaled_kps_v0 = soft_kp_vt[0].clone()\n",
    "            scaled_kps_v0[..., 0] *= scale_x\n",
    "            scaled_kps_v0[..., 1] *= scale_y\n",
    "\n",
    "            scaled_kps_v1 = soft_kp_vt[0].clone()\n",
    "            scaled_kps_v1[..., 0] *= scale_x\n",
    "            scaled_kps_v1[..., 1] *= scale_y\n",
    "\n",
    "            # Clamp images for visualization\n",
    "            v0_img = v1[0].clamp(0, 1)\n",
    "            v1_img = vt[0].clamp(0, 1)\n",
    "            v1_pred_img = vt_pred[0].clamp(0, 1)\n",
    "\n",
    "            # Overlay keypoints\n",
    "            vis_v0 = draw_keypoints_on_image(v0_img, scaled_kps_v0, color='b')  # Blue keypoints\n",
    "            vis_v1 = draw_keypoints_on_image(v1_img, scaled_kps_v1, color='r')  # Red keypoints\n",
    "            vis_pred = draw_keypoints_on_image(v1_pred_img, scaled_kps_v1, color='r')\n",
    "\n",
    "            # Combine visuals\n",
    "            grid = make_grid([vis_v0, vis_v1, vis_pred])\n",
    "            self.logger.experiment.add_image(\"val/visualize_key_reconstruction\", grid, self.current_epoch)\n",
    "\n",
    "            # Log heatmaps of v1\n",
    "            sample_heatmaps = vt_heatmaps[0].unsqueeze(1)  # [K, 1, H, W]\n",
    "            heatmap_grid = make_grid(sample_heatmaps, nrow=10, normalize=True, scale_each=True)\n",
    "            self.logger.experiment.add_image(\"val/heatmaps_v1\", heatmap_grid, self.current_epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def keypoint_loss(self, vt, vt_pred, v0_heat, vt_heat):\n",
    "\n",
    "        mse = nn.functional.mse_loss(vt, vt_pred)\n",
    "\n",
    "\n",
    "        # condensation_loss_1 = (v0_heat.amax(dim=(-1,-2)) - v0_heat.mean(dim=(-1,-2))).mean()\n",
    "        # condensation_loss_t = (vt_heat.amax(dim=(-1,-2)) - vt_heat.mean(dim=(-1,-2))).mean()\n",
    "        # condensation_loss = -(condensation_loss_1 + condensation_loss_t)/2\n",
    "        condensation_loss_1 = self.condensation_loss_entropy(v0_heat)\n",
    "        condensation_loss_t = self.condensation_loss_entropy(vt_heat)\n",
    "        condensation_loss = (condensation_loss_1 + condensation_loss_t)/2\n",
    "\n",
    "        return mse+self.alpha*condensation_loss, mse, condensation_loss\n",
    "    \n",
    "    def condensation_loss_entropy(self, heatmaps):\n",
    "        # heatmaps: [B, K, H, W] (softmaxed)\n",
    "        eps = 1e-8\n",
    "        log_h = torch.log(heatmaps + eps)\n",
    "        entropy = -torch.sum(heatmaps * log_h, dim=(-2, -1))  # [B, K]\n",
    "        return entropy.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cd7819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4982)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,64,32,32)\n",
    "(x.amax(dim=(-1,-2)) - x.mean(dim=(-1,-2))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0bcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class MovingMNISTPairs(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.dataset = MNIST(root=root, train=train, download=True)\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.canvas_size = 128\n",
    "        self.digit_size = 28\n",
    "        self.max_pos = self.canvas_size - self.digit_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]\n",
    "        img = self.transform(img)  # [1, 28, 28]\n",
    "\n",
    "        # Random top-left position where digit fits\n",
    "        x = random.randint(0, self.max_pos)\n",
    "        y = random.randint(0, self.max_pos)\n",
    "\n",
    "        # Create black canvas and place digit\n",
    "        canvas = torch.zeros(1, self.canvas_size, self.canvas_size)\n",
    "        canvas[:, y:y+self.digit_size, x:x+self.digit_size] = img\n",
    "\n",
    "        # Simulate motion (safe shift)\n",
    "        dx = random.randint(-4, 4)\n",
    "        dy = random.randint(-4, 4)\n",
    "        new_x = min(max(x + dx, 0), self.max_pos)\n",
    "        new_y = min(max(y + dy, 0), self.max_pos)\n",
    "\n",
    "        moved = torch.zeros_like(canvas)\n",
    "        moved[:, new_y:new_y+self.digit_size, new_x:new_x+self.digit_size] = img\n",
    "\n",
    "        return canvas.expand(3, -1, -1), moved.expand(3, -1, -1)  # Convert to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb90125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_dataloaders(data_root=\"./data\", batch_size=8):\n",
    "    full_train = MovingMNISTPairs(data_root, train=True)\n",
    "    val_size = int(0.1 * len(full_train))\n",
    "    train_size = len(full_train) - val_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c00932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CameraSequentialPairs(Dataset):\n",
    "    def __init__(self, root, transform=None, min_offset=1, max_offset=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory containing camera folders.\n",
    "            transform: Transformations to apply to each image.\n",
    "            min_offset (int): Minimum frame difference between pairs.\n",
    "            max_offset (int): Maximum frame difference between pairs.\n",
    "        \"\"\"\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.min_offset = min_offset\n",
    "        self.max_offset = max_offset\n",
    "        self.sequences = []\n",
    "\n",
    "        # List all camera directories\n",
    "        camera_dirs = sorted([os.path.join(root, d) for d in os.listdir(root)\n",
    "                              if os.path.isdir(os.path.join(root, d))])\n",
    "        \n",
    "        # For each camera directory, get sorted image paths (using numeric sorting)\n",
    "        for cam_dir in camera_dirs:\n",
    "            image_dir = os.path.join(cam_dir, \"images\")\n",
    "            if not os.path.exists(image_dir):\n",
    "                continue\n",
    "            image_files = sorted(\n",
    "                [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')],\n",
    "                key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[-1])\n",
    "            )\n",
    "            if len(image_files) > 1:\n",
    "                self.sequences.append(image_files)\n",
    "        \n",
    "        # Build cumulative lengths for indexing purposes\n",
    "        self.cumulative_lengths = []\n",
    "        total = 0\n",
    "        for seq in self.sequences:\n",
    "            # Each valid pair comes from a starting image (all except the last)\n",
    "            total += len(seq) - 1\n",
    "            self.cumulative_lengths.append(total)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_lengths[-1] if self.cumulative_lengths else 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    # Find which sequence to use\n",
    "        seq_idx = 0\n",
    "        while idx >= self.cumulative_lengths[seq_idx]:\n",
    "            seq_idx += 1\n",
    "        seq = self.sequences[seq_idx]\n",
    "        \n",
    "        local_idx = idx if seq_idx == 0 else idx - self.cumulative_lengths[seq_idx - 1]\n",
    "        \n",
    "        # Check that the index is valid\n",
    "        if local_idx >= len(seq) - 1:\n",
    "            raise IndexError(\"Local index out of range for the sequence\")\n",
    "        \n",
    "        img1_path = seq[local_idx]\n",
    "\n",
    "        # Compute the maximum valid offset\n",
    "        remaining = len(seq) - 1 - local_idx\n",
    "        effective_max_offset = min(self.max_offset, remaining)\n",
    "        \n",
    "        # In case effective_max_offset < min_offset, use effective_max_offset\n",
    "        # Otherwise, randomly sample\n",
    "        offset = effective_max_offset  # or if you want random:\n",
    "        # offset = random.randint(self.min_offset, effective_max_offset)\n",
    "\n",
    "        img2_idx = local_idx + offset\n",
    "        \n",
    "        img2_path = seq[img2_idx]\n",
    "\n",
    "        try:\n",
    "            img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "            img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images: {img1_path} or {img2_path}: {e}\")\n",
    "            # Optionally, raise an error here or return a default dummy tensor\n",
    "            raise e\n",
    "        \n",
    "        return self.transform(img1), self.transform(img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b08efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset pairs: 199998\n",
      "Validation dataset pairs: 19998\n",
      "Batch shapes: torch.Size([16, 3, 128, 128]) torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assuming CameraSequentialPairs is defined as in the previous code block.\n",
    "# For example:\n",
    "# class CameraSequentialPairs(Dataset):\n",
    "#     ... (code from previous message) ...\n",
    "\n",
    "def get_camera_dataloaders(data_root=\"./dataset_00\", batch_size=8, min_offset=5, max_offset=30, num_workers=8):\n",
    "    # For training, we use the \"train\" subfolder within the data root.\n",
    "    train_dataset = CameraSequentialPairs(\n",
    "        root=os.path.join(data_root, \"train\"),\n",
    "        transform=transforms.ToTensor(),\n",
    "        min_offset=min_offset,\n",
    "        max_offset=max_offset\n",
    "    )\n",
    "    \n",
    "    # Similarly, for validation, use the \"val\" subfolder.\n",
    "    val_dataset = CameraSequentialPairs(\n",
    "        root=os.path.join(data_root, \"val\"),\n",
    "        transform=transforms.ToTensor(),\n",
    "        min_offset=min_offset,\n",
    "        max_offset=max_offset\n",
    "    )\n",
    "    \n",
    "    # Optionally, print out dataset sizes for debugging\n",
    "    print(\"Training dataset pairs:\", len(train_dataset))\n",
    "    print(\"Validation dataset pairs:\", len(val_dataset))\n",
    "    \n",
    "    # Build DataLoaders with shuffling for training and no shuffle for validation.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Example usage:\n",
    "train_loader, val_loader = get_camera_dataloaders(\n",
    "    data_root=\"./dataset_00\", batch_size=16, min_offset=1, max_offset=5, num_workers=8\n",
    ")\n",
    "\n",
    "# For testing, iterate over one batch:\n",
    "for img1, img2 in train_loader:\n",
    "    print(\"Batch shapes:\", img1.shape, img2.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3440bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset pairs: 199998\n",
      "Validation dataset pairs: 19998\n",
      "12500\n",
      "torch.Size([16, 3, 128, 128]) torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_camera_dataloaders(data_root=\"./dataset_00\", batch_size=16)\n",
    "print(len(train_loader))\n",
    "for img1, img2 in train_loader:\n",
    "    print(img1.shape, img2.shape)  # e.g., [16, 3, H, W]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77bce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = L.Trainer(max_epochs=200, callbacks=[AlphaScheduler()])\n",
    "# model = LitKeypointDetector(KeypointPredictor(100), ImageEncoder(), ImageDecoder(64, 100))\n",
    "\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4fc3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=200, nhead=8, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "# out = transformer_encoder(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ce0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceKeypointDetector(nn.Module):\n",
    "    def __init__(self, keypoint_detector):\n",
    "        super().__init__()\n",
    "        self.keypoint_detector = keypoint_detector  # This is your existing 4D keypoint model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: input tensor of shape (B, T, C, H, W)\n",
    "        Returns:\n",
    "          Output: tensor of shape (B, T, num_keypoints, 2)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        # Flatten batch and sequence dimensions\n",
    "        x_flat = x.view(B * T, C, H, W)\n",
    "        # Process with keypoint detector (assumed to output (B*T, num_keypoints, 2))\n",
    "        keypoints_flat = self.keypoint_detector(x_flat)[0]  # For example, if keypoint_detector returns (grid_keypoints, soft_keypoints, heatmaps)\n",
    "        # Reshape the output back to (B, T, num_keypoints, 2)\n",
    "        keypoints = keypoints_flat.view(B, T, -1, 2)\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c3ca6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarikipati7/.env/ssta_env/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100, 200])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "seq_detector = SequenceKeypointDetector(KeypointPredictor(100))\n",
    "# Sample input: (64, 100, 3, 128, 128)\n",
    "input_tensor = torch.randn(8, 100, 3, 128, 128)\n",
    "output_keypoints = seq_detector(input_tensor)\n",
    "transformer_input = output_keypoints.view(8,100,-1)\n",
    "print(transformer_input.size())  # Should print torch.Size([64, 100, 100, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98984c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = transformer_encoder(transformer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c123bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a602a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses.masked_mse import masked_mse\n",
    "import torchvision.utils as vutils\n",
    "class LitPredictorT2NOD(L.LightningModule):\n",
    "    def __init__(self, image_encoder, seq_keypoint_encoder, transformer_encoder, image_decoder):\n",
    "        super().__init__()\n",
    "        self.seq_keypoint_encoder = seq_keypoint_encoder\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.image_decoder = image_decoder\n",
    "        D = 200  # flatten dim: T × (K × 2)\n",
    "        T = 100\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, T, D))  # [1, T, D]\n",
    "\n",
    "\n",
    "\n",
    "        # self.seq_keypoint_encoder.eval()\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seq_images, t2nod= batch\n",
    "        # with torch.no_grad():\n",
    "        seq_keypoints = self.seq_keypoint_encoder(seq_images)\n",
    "\n",
    "        B, T, num_kps, _ = seq_keypoints.shape\n",
    "        flat_seq_keypoints = seq_keypoints.view(B, T, -1)\n",
    "        flat_seq_keypoints = flat_seq_keypoints + self.pos_embed[:, :T]\n",
    "\n",
    "        flat_pred_kepoints = self.transformer_encoder(flat_seq_keypoints)\n",
    "\n",
    "        pred_keypoints = flat_pred_kepoints.view(B,T*num_kps,2)\n",
    "        last_frame = seq_images[:, -1, :, :, :]\n",
    "        features = self.image_encoder(last_frame)\n",
    "        out = self.image_decoder(features, pred_keypoints)\n",
    "        \n",
    "        total_loss, bce, t2no_mse, t2nd_mse = masked_mse(out, t2nod)\n",
    "        self.log(\"train/masked_mse\", total_loss)\n",
    "        self.log(\"train/bce\", bce)\n",
    "        self.log(\"train/t2no_mse\", t2no_mse)\n",
    "        self.log(\"train/t2nd_mse\", t2nd_mse)\n",
    "        return total_loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seq_images, t2nod = batch\n",
    "        with torch.no_grad():\n",
    "            seq_keypoints = self.seq_keypoint_encoder(seq_images)\n",
    "\n",
    "        B, T, num_kps, _ = seq_keypoints.shape\n",
    "        flat_seq_keypoints = seq_keypoints.view(B, T, -1)\n",
    "        flat_pred_kepoints = self.transformer_encoder(flat_seq_keypoints)\n",
    "        pred_keypoints = flat_pred_kepoints.view(B, T * num_kps, 2)\n",
    "\n",
    "        last_frame = seq_images[:, -1]  # [B, 3, H, W]\n",
    "        features = self.image_encoder(last_frame)\n",
    "        out = self.image_decoder(features, pred_keypoints)  # [B, 3, H, W]\n",
    "\n",
    "        total_loss, bce, t2no_mse, t2nd_mse, mask = masked_mse(out, t2nod, return_mask=True)\n",
    "\n",
    "        self.log(\"val/masked_mse\", total_loss)\n",
    "        self.log(\"val/bce\", bce)\n",
    "        self.log(\"val/t2no_mse\", t2no_mse)\n",
    "        self.log(\"val/t2nd_mse\", t2nd_mse)\n",
    "\n",
    "        if batch_idx == 0:  # Log only the first batch per epoch\n",
    "            grid = self.visualize_predictions_for_tensorboard(\n",
    "                last_frame[0],      # RGB image\n",
    "                t2nod[0],           # [2, H, W] GT\n",
    "                out[0],             # [3, H, W] prediction\n",
    "                mask,             # [1, H, W]\n",
    "            )\n",
    "            self.logger.experiment.add_image(\"val/sample_grid\", grid, self.global_step)\n",
    "\n",
    "    def visualize_predictions_for_tensorboard(self, rgb_img, gt, pred, mask):\n",
    "        \"\"\"\n",
    "        Returns a grid of shape [3, H, 7*W] for logging as an image.\n",
    "        \"\"\"\n",
    "        def to_3ch(x):\n",
    "            if x.ndim == 2:\n",
    "                return x.unsqueeze(0).repeat(3, 1, 1)\n",
    "            elif x.shape[0] == 1:\n",
    "                return x.repeat(3, 1, 1)\n",
    "            return x\n",
    "\n",
    "        def normalize(x):\n",
    "            return (x - x.min()) / (x.max() - x.min() + 1e-5)\n",
    "\n",
    "        rgb_img = normalize(rgb_img)\n",
    "\n",
    "        t2no_mask     = (gt[0] != 1).float()\n",
    "        gt_t2no       = gt[0]\n",
    "        gt_t2nd       = gt[1]\n",
    "        print(\"GT t2no unique values:\", gt[0].unique())\n",
    "        print(\"GT t2no mask sum:\", (gt[0] != 1).float().unique())\n",
    "        pred_mask_asis = normalize(torch.sigmoid(pred[0]))\n",
    "        pred_mask = (torch.sigmoid(pred[0]) > 0.5).float()\n",
    "\n",
    "        pred_t2no     = pred[1] * mask[0] + (~mask[0])\n",
    "        pred_t2nd     = pred[2] * mask[0] + (~mask[0])\n",
    "\n",
    "        tiles = [\n",
    "            to_3ch(rgb_img),\n",
    "            to_3ch(t2no_mask),\n",
    "            to_3ch(normalize(gt_t2no)),\n",
    "            to_3ch(normalize(gt_t2nd)),\n",
    "            to_3ch(pred_mask),\n",
    "            to_3ch(pred_mask_asis),\n",
    "            to_3ch(normalize(pred_t2no)),\n",
    "            to_3ch(normalize(pred_t2nd)),\n",
    "            to_3ch(normalize(pred[1])),\n",
    "            to_3ch(normalize(pred[2]))\n",
    "        ]\n",
    "\n",
    "        grid = vutils.make_grid(tiles, nrow=7)\n",
    "        return grid\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d45e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def logsumexp_pooling(heatmaps, group_size=100, beta=10.0):\n",
    "    \"\"\"\n",
    "    Consolidate heatmaps using LogSumExp pooling.\n",
    "\n",
    "    Args:\n",
    "        heatmaps: Tensor of shape (B, K, H, W) where K = num_keypoints\n",
    "        num_groups: Number of final heatmaps desired\n",
    "        beta: Temperature parameter for LogSumExp\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (B, num_groups, H, W)\n",
    "    \"\"\"\n",
    "    B, K, H, W = heatmaps.shape\n",
    "\n",
    "    # Reshape: (B, num_groups, group_size, H, W)\n",
    "    grouped = heatmaps.view(B, K//group_size, group_size, H, W)\n",
    "\n",
    "    # LogSumExp pooling over the group dimension\n",
    "    pooled = (1.0 / beta) * torch.logsumexp(beta * grouped, dim=2)  # shape: (B, num_groups, H, W)\n",
    "\n",
    "    return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fb97d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have 10,000 keypoint heatmaps of size 32x32\n",
    "keypoints = torch.randn(8, 20000)  # Batch of 8\n",
    "keypoints = keypoints.view(8,10000,-1)\n",
    "features = torch.randn(8, 32, 32, 32)  # Batch of 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d189a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decoder = ImageDecoder(32, 1000, 3, condense=True)\n",
    "decoder(features, keypoints).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8ea162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DummySequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, sequence_length=100, channels=3, height=128, width=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset.\n",
    "            sequence_length (int): Number of images per sequence.\n",
    "            channels (int): Number of color channels (typically 3 for RGB).\n",
    "            height (int): Height of each image.\n",
    "            width (int): Width of each image.\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a random image sequence: shape [T, 3, 128, 128]\n",
    "        seq_images = torch.rand(self.sequence_length, self.channels, self.height, self.width)\n",
    "        # Generate a dummy target image (t2nod) for the final frame: shape [3, 128, 128]\n",
    "        # For example, you could imagine these values are normalized between 0 and 1.\n",
    "        t2nod = torch.rand(2, self.height, self.width)\n",
    "        return seq_images, t2nod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c778dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset pairs: 100\n",
      "Validation dataset pairs: 100\n",
      "torch.Size([16, 100, 3, 128, 128]) torch.Size([16, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "def get_camera_dataloaders(batch_size=8, min_offset=5, max_offset=30, num_workers=8):\n",
    "    train_dataset = DummySequenceDataset(\n",
    "        num_samples=100, sequence_length=100, channels=3, height=128, width=128\n",
    "    )\n",
    "    \n",
    "    val_dataset = DummySequenceDataset(\n",
    "        num_samples=100, sequence_length=100, channels=3, height=128, width=128\n",
    "    )\n",
    "    \n",
    "    print(\"Training dataset pairs:\", len(train_dataset))\n",
    "    print(\"Validation dataset pairs:\", len(val_dataset))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = get_camera_dataloaders(\n",
    "    batch_size=16, min_offset=1, max_offset=5, num_workers=8\n",
    ")\n",
    "\n",
    "for each in train_loader:\n",
    "    break\n",
    "print(each[0].size(), each[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6190f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=200, nhead=8, batch_first=True)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "    \n",
    "# trainer = L.Trainer(max_epochs=200, callbacks=[AlphaScheduler()])\n",
    "# model = LitPredictorT2NOD(ImageEncoder(), SequenceKeypointDetector(KeypointPredictor(100)), transformer_encoder, ImageDecoder(64, 1000, 3, condense=True))\n",
    "\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13eef117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: torch.Size([20, 3, 128, 128])\n",
      "Target shape: torch.Size([2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import re\n",
    "\n",
    "def natural_sort(l):\n",
    "    return sorted(l, key=lambda s: [int(text) if text.isdigit() else text.lower()\n",
    "                                    for text in re.split(r'(\\d+)', s)])\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 root_dir, \n",
    "                 mode='train', \n",
    "                 sequence_length=5,  \n",
    "                 input_transform=None, \n",
    "                 target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset folder (e.g., 'dataset_00').\n",
    "            mode (str): 'train' or 'val'.\n",
    "            sequence_length (int): Number of consecutive images in the input sequence.\n",
    "            input_transform (callable, optional): Transformation to be applied to each input image.\n",
    "            target_transform (callable, optional): Transformation to be applied to each target image.\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.samples = []  # Will hold dictionaries with sample info.\n",
    "        mode_dir = os.path.join(root_dir, mode)\n",
    "        \n",
    "        # Get each camera directory (e.g., camera_0, camera_1, etc.)\n",
    "        camera_dirs = sorted([d for d in os.listdir(mode_dir) if os.path.isdir(os.path.join(mode_dir, d))])\n",
    "        \n",
    "        for camera in camera_dirs:\n",
    "            camera_path = os.path.join(mode_dir, camera)\n",
    "            images_dir = os.path.join(camera_path, 'images')\n",
    "            t2no_dir   = os.path.join(camera_path, 't2no')\n",
    "            t2nd_dir   = os.path.join(camera_path, 't2nd')\n",
    "            \n",
    "            # List and sort file names in each folder\n",
    "            image_files = natural_sort(os.listdir(images_dir))\n",
    "            t2no_files  = natural_sort(os.listdir(t2no_dir))\n",
    "            t2nd_files  = natural_sort(os.listdir(t2nd_dir))\n",
    "            \n",
    "            # Check that all three folders contain the same number of files.\n",
    "            if not (len(image_files) == len(t2no_files) == len(t2nd_files)):\n",
    "                raise ValueError(f\"Mismatch in file counts in camera folder: {camera}\")\n",
    "            \n",
    "            num_frames = len(image_files)\n",
    "            # Only use valid starting indices where the full sequence exists.\n",
    "            for start_idx in range(num_frames - sequence_length + 1):\n",
    "                self.samples.append({\n",
    "                    \"images_dir\": images_dir,\n",
    "                    \"t2no_dir\": t2no_dir,\n",
    "                    \"t2nd_dir\": t2nd_dir,\n",
    "                    \"start_idx\": start_idx,\n",
    "                    \"file_list\": image_files  # Assuming the same ordering applies for all directories.\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.samples[idx]\n",
    "        images_dir = sample_info[\"images_dir\"]\n",
    "        t2no_dir   = sample_info[\"t2no_dir\"]\n",
    "        t2nd_dir   = sample_info[\"t2nd_dir\"]\n",
    "        start_idx  = sample_info[\"start_idx\"]\n",
    "        file_list  = sample_info[\"file_list\"]\n",
    "        \n",
    "        # Load the input sequence as RGB images.\n",
    "        input_sequence = []\n",
    "        for i in range(start_idx, start_idx + self.sequence_length):\n",
    "            img_path = os.path.join(images_dir, file_list[i])\n",
    "            # Open the image in RGB mode to retain three channels.\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.input_transform:\n",
    "                img = self.input_transform(img)\n",
    "            else:\n",
    "                # Default: convert image to tensor (C x H x W) with values in [0, 1]\n",
    "                img = T.ToTensor()(img)\n",
    "            input_sequence.append(img)\n",
    "        \n",
    "        # Stack into a tensor of shape [sequence_length, 3, H, W]\n",
    "        input_sequence = torch.stack(input_sequence, dim=0)\n",
    "        \n",
    "        # For the target, choose the frame corresponding to the last image in the sequence.\n",
    "        target_idx = start_idx + self.sequence_length - 1\n",
    "        t2no_path = os.path.join(t2no_dir, file_list[target_idx])\n",
    "        t2nd_path = os.path.join(t2nd_dir, file_list[target_idx])\n",
    "        \n",
    "        # Load target images in grayscale.\n",
    "        t2no = Image.open(t2no_path).convert('L')\n",
    "        t2nd = Image.open(t2nd_path).convert('L')\n",
    "        \n",
    "        # Resize the targets to 128x128 pixels.\n",
    "        resize_transform = T.Resize((128, 128))\n",
    "        t2no = resize_transform(t2no)\n",
    "        t2nd = resize_transform(t2nd)\n",
    "\n",
    "        t2no = torch.from_numpy(np.array(t2no)).float().unsqueeze(0) / 100.0 \n",
    "        t2nd = torch.from_numpy(np.array(t2nd)).float().unsqueeze(0) / 100.0\n",
    "        \n",
    "        # Concatenate the two target images along the channel dimension.\n",
    "        target = torch.cat([t2no, t2nd], dim=0)  # Expected shape: [2, 128, 128]\n",
    "        \n",
    "        return input_sequence, target\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Define transforms if desired.\n",
    "    input_transform = T.Compose([\n",
    "        # Example: You can add T.Resize, T.RandomCrop, etc.\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    target_transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Create dataset instances for training and validation.\n",
    "    dataset_root = 'dataset_00'  # Replace with your dataset path.\n",
    "    train_dataset = SequenceDataset(root_dir=dataset_root, \n",
    "                                    mode='train', \n",
    "                                    sequence_length=20, \n",
    "                                    input_transform=input_transform, \n",
    "                                    target_transform=target_transform)\n",
    "    \n",
    "    val_dataset = SequenceDataset(root_dir=dataset_root, \n",
    "                                  mode='val', \n",
    "                                  sequence_length=20, \n",
    "                                  input_transform=input_transform, \n",
    "                                  target_transform=target_transform)\n",
    "    \n",
    "    # Test by loading one sample.\n",
    "    sample_in, sample_target = train_dataset[0]\n",
    "    print(\"Input sequence shape:\", sample_in.shape)      # Expected: [sequence_length, 3, H, W]\n",
    "    print(\"Target shape:\", sample_target.shape)            # Expected: [2, 128, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2529034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# # Assuming you already defined SequenceDataset and created an instance\n",
    "# # For example:\n",
    "# # dataset_root = 'dataset_00'\n",
    "# # train_dataset = SequenceDataset(root_dir=dataset_root, mode='train', sequence_length=5, \n",
    "# #                                 input_transform=input_transform, target_transform=target_transform)\n",
    "\n",
    "# # Get one sample from the training dataset\n",
    "# input_sequence, target = train_dataset[0]\n",
    "\n",
    "# # Visualize the input sequence images.\n",
    "# # input_sequence shape: [sequence_length, 3, H, W]\n",
    "# for i in range(input_sequence.shape[0]):\n",
    "#     # Retrieve the i-th image, which is a tensor of shape [3, H, W]\n",
    "#     img_tensor = input_sequence[i]\n",
    "#     # Convert the tensor to a numpy array and transpose to [H, W, 3] for matplotlib\n",
    "#     img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.imshow(img_np)\n",
    "#     plt.title(f\"Input Sequence Frame {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize the target images.\n",
    "# # target shape: [2, 128, 128]; each channel is a grayscale image.\n",
    "# # We'll display them side by side in separate figures.\n",
    "# target_names = ['t2no', 't2nd']\n",
    "# for j in range(target.shape[0]):\n",
    "#     target_img = target[j]  # tensor shape [128, 128]\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.imshow(target_img.numpy().squeeze(), cmap='gray')\n",
    "#     plt.title(f\"Target Image: {target_names[j]}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a53260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_3/checkpoints/epoch=4-step=124880.ckpt\n",
      "/home/sgarikipati7/.env/ssta_env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_3/checkpoints' to '/home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_4/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type                     | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | seq_keypoint_encoder | SequenceKeypointDetector | 164 K  | train\n",
      "1 | transformer_encoder  | TransformerEncoder       | 5.9 M  | train\n",
      "2 | image_encoder        | ImageEncoder             | 157 K  | train\n",
      "3 | image_decoder        | ImageDecoder             | 190 K  | train\n",
      "  | other params         | n/a                      | 20.0 K | n/a  \n",
      "--------------------------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.722    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_3/checkpoints/epoch=4-step=124880.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]GT t2no unique values: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
      "        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n",
      "        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n",
      "        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n",
      "        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n",
      "        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n",
      "        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n",
      "        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n",
      "        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n",
      "        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n",
      "        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n",
      "        0.9900, 1.0000], device='cuda:0')\n",
      "GT t2no mask sum: tensor([0., 1.], device='cuda:0')\n",
      "Epoch 5:  34%|███▍      | 8494/24976 [17:16<33:31,  8.19it/s, v_num=4]     "
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_ROOT = \"dataset_00\"\n",
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 8\n",
    "MAX_EPOCHS = 200\n",
    "\n",
    "# ------------------------------\n",
    "# Transforms\n",
    "# ------------------------------\n",
    "input_transform = T.Compose([\n",
    "    T.Resize((128, 128)),  # Or whatever size your model expects\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    lambda x: x / 100.0\n",
    "])\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset & DataLoaders\n",
    "# ------------------------------\n",
    "train_dataset = SequenceDataset(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    mode=\"train\",\n",
    "    sequence_length=SEQ_LEN,\n",
    "    input_transform=input_transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "val_dataset = SequenceDataset(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    mode=\"val\",\n",
    "    sequence_length=SEQ_LEN,\n",
    "    input_transform=input_transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ------------------------------\n",
    "# Model Initialization\n",
    "# ------------------------------\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=200, nhead=8, batch_first=True)\n",
    "\n",
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "seq_detector = SequenceKeypointDetector(KeypointPredictor(100))\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "checkpoint = \"/home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_0/checkpoints/epoch=57-step=725000.ckpt\"\n",
    "seq_keypoint_encoder = SequenceKeypointDetector(KeypointPredictor(100))\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "image_decoder = ImageDecoder(64,100,3, condense=True)\n",
    "\n",
    "model = LitPredictorT2NOD(\n",
    "    image_encoder=image_encoder,\n",
    "    seq_keypoint_encoder=seq_keypoint_encoder,\n",
    "    transformer_encoder=transformer_encoder,\n",
    "    image_decoder=image_decoder\n",
    ")\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(model, train_loader, val_loader,ckpt_path=\"/home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_3/checkpoints/epoch=4-step=124880.ckpt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
