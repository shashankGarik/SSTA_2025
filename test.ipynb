{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f86a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149436e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarikipati7/.env/ssta_env/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "x = torch.rand(1,3,128,128)\n",
    "model = KeypointPredictor()\n",
    "out, soft, m = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079d8536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,64,32,32)\n",
    "y = torch.rand(1,100,2)\n",
    "\n",
    "model = ImageDecoder(64, 100, 3)\n",
    "out = model(x,y)\n",
    "out.shape\n",
    "\n",
    "model = ImageEncoder()\n",
    "out = model(torch.rand(1,3,128,128))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e03aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "class AlphaScheduler(Callback):\n",
    "    def __init__(self, warmup_epochs=10, ramp_epochs=20, final_alpha=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            warmup_epochs: Number of epochs to keep alpha = 0\n",
    "            ramp_epochs: Number of epochs to linearly ramp up alpha after warmup\n",
    "            final_alpha: Target alpha value\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.ramp_epochs = ramp_epochs\n",
    "        self.final_alpha = final_alpha\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "\n",
    "        if epoch < self.warmup_epochs:\n",
    "            alpha = 0.0\n",
    "        elif epoch < self.warmup_epochs + self.ramp_epochs:\n",
    "            # Linear ramp from 0 to final_alpha\n",
    "            ramp_progress = (epoch - self.warmup_epochs) / self.ramp_epochs\n",
    "            alpha = ramp_progress * self.final_alpha\n",
    "        else:\n",
    "            alpha = self.final_alpha\n",
    "\n",
    "        pl_module.alpha = alpha\n",
    "\n",
    "def draw_keypoints_on_image(image_tensor, keypoints, color='r'):\n",
    "    \"\"\"\n",
    "    image_tensor: [3, H, W] (float in [0,1])\n",
    "    keypoints: [N, 2] with (x, y) format in image coordinates\n",
    "    \"\"\"\n",
    "    image = TF.to_pil_image(image_tensor.cpu())\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    for x, y in keypoints.cpu():\n",
    "        plt.scatter(x, y, c=color, s=10)\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='jpeg', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    pil_image = Image.open(buf)\n",
    "    return TF.to_tensor(pil_image)\n",
    "    \n",
    "class LitKeypointDetector(L.LightningModule):\n",
    "    def __init__(self, keypoint_encoder, feature_encoder, feature_decoder):\n",
    "        super().__init__()\n",
    "        self.keypoint_generator = keypoint_encoder\n",
    "        self.feature_encoder = feature_encoder\n",
    "        self.image_reconstructor = feature_decoder\n",
    "        self.alpha = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        v1, vt = batch\n",
    "        \n",
    "        v1_features = self.feature_encoder(v1)\n",
    "\n",
    "        _, _, v1_heatmaps = self.keypoint_generator(v1)\n",
    "        _, vt_soft_keypoints, vt_heatmaps = self.keypoint_generator(vt)\n",
    "\n",
    "        vt_pred = self.image_reconstructor(v1_features, vt_soft_keypoints)\n",
    "        loss, mse, condens = self.keypoint_loss(vt, vt_pred, v1_heatmaps, vt_heatmaps)\n",
    "        self.log(\"train/total_loss\", loss)\n",
    "        self.log(\"train/mse\", mse)\n",
    "        self.log(\"train/condensation_loss\", condens)\n",
    "        self.log(\"alpha\", self.alpha)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        v1, vt = batch\n",
    "\n",
    "        # Encode features from v0\n",
    "        v1_features = self.feature_encoder(v1)\n",
    "\n",
    "        # Get keypoints from v0 and v1 separately\n",
    "        _, soft_kp_vt, v1_heatmaps = self.keypoint_generator(v1)\n",
    "        _, soft_kp_vt, vt_heatmaps = self.keypoint_generator(vt)\n",
    "\n",
    "        # Predict v1 using v0 features + v1 keypoints\n",
    "        vt_pred = self.image_reconstructor(v1_features, soft_kp_vt)\n",
    "        loss, mse, condens = self.keypoint_loss(vt, vt_pred, v1_heatmaps, vt_heatmaps)\n",
    "\n",
    "        self.log(\"val/total_loss\", loss)\n",
    "        self.log(\"val/mse\", mse)\n",
    "        self.log(\"val/condensation_loss\", condens)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            B, C, img_H, img_W = v1.shape\n",
    "            _, _, heatmap_H, heatmap_W = vt_heatmaps.shape\n",
    "\n",
    "            scale_x = img_W / heatmap_W\n",
    "            scale_y = img_H / heatmap_H\n",
    "\n",
    "            # Rescale keypoints\n",
    "            scaled_kps_v0 = soft_kp_vt[0].clone()\n",
    "            scaled_kps_v0[..., 0] *= scale_x\n",
    "            scaled_kps_v0[..., 1] *= scale_y\n",
    "\n",
    "            scaled_kps_v1 = soft_kp_vt[0].clone()\n",
    "            scaled_kps_v1[..., 0] *= scale_x\n",
    "            scaled_kps_v1[..., 1] *= scale_y\n",
    "\n",
    "            # Clamp images for visualization\n",
    "            v0_img = v1[0].clamp(0, 1)\n",
    "            v1_img = vt[0].clamp(0, 1)\n",
    "            v1_pred_img = vt_pred[0].clamp(0, 1)\n",
    "\n",
    "            # Overlay keypoints\n",
    "            vis_v0 = draw_keypoints_on_image(v0_img, scaled_kps_v0, color='b')  # Blue keypoints\n",
    "            vis_v1 = draw_keypoints_on_image(v1_img, scaled_kps_v1, color='r')  # Red keypoints\n",
    "            vis_pred = draw_keypoints_on_image(v1_pred_img, scaled_kps_v1, color='r')\n",
    "\n",
    "            # Combine visuals\n",
    "            grid = make_grid([vis_v0, vis_v1, vis_pred])\n",
    "            self.logger.experiment.add_image(\"val/visualize_key_reconstruction\", grid, self.current_epoch)\n",
    "\n",
    "            # Log heatmaps of v1\n",
    "            sample_heatmaps = vt_heatmaps[0].unsqueeze(1)  # [K, 1, H, W]\n",
    "            heatmap_grid = make_grid(sample_heatmaps, nrow=10, normalize=True, scale_each=True)\n",
    "            self.logger.experiment.add_image(\"val/heatmaps_v1\", heatmap_grid, self.current_epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def keypoint_loss(self, vt, vt_pred, v0_heat, vt_heat):\n",
    "\n",
    "        mse = nn.functional.mse_loss(vt, vt_pred)\n",
    "\n",
    "\n",
    "        # condensation_loss_1 = (v0_heat.amax(dim=(-1,-2)) - v0_heat.mean(dim=(-1,-2))).mean()\n",
    "        # condensation_loss_t = (vt_heat.amax(dim=(-1,-2)) - vt_heat.mean(dim=(-1,-2))).mean()\n",
    "        # condensation_loss = -(condensation_loss_1 + condensation_loss_t)/2\n",
    "        condensation_loss_1 = self.condensation_loss_entropy(v0_heat)\n",
    "        condensation_loss_t = self.condensation_loss_entropy(vt_heat)\n",
    "        condensation_loss = (condensation_loss_1 + condensation_loss_t)/2\n",
    "\n",
    "        return mse+self.alpha*condensation_loss, mse, condensation_loss\n",
    "    \n",
    "    def condensation_loss_entropy(self, heatmaps):\n",
    "        # heatmaps: [B, K, H, W] (softmaxed)\n",
    "        eps = 1e-8\n",
    "        log_h = torch.log(heatmaps + eps)\n",
    "        entropy = -torch.sum(heatmaps * log_h, dim=(-2, -1))  # [B, K]\n",
    "        return entropy.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cd7819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4991)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,64,32,32)\n",
    "(x.amax(dim=(-1,-2)) - x.mean(dim=(-1,-2))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0bcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class MovingMNISTPairs(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.dataset = MNIST(root=root, train=train, download=True)\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.canvas_size = 128\n",
    "        self.digit_size = 28\n",
    "        self.max_pos = self.canvas_size - self.digit_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]\n",
    "        img = self.transform(img)  # [1, 28, 28]\n",
    "\n",
    "        # Random top-left position where digit fits\n",
    "        x = random.randint(0, self.max_pos)\n",
    "        y = random.randint(0, self.max_pos)\n",
    "\n",
    "        # Create black canvas and place digit\n",
    "        canvas = torch.zeros(1, self.canvas_size, self.canvas_size)\n",
    "        canvas[:, y:y+self.digit_size, x:x+self.digit_size] = img\n",
    "\n",
    "        # Simulate motion (safe shift)\n",
    "        dx = random.randint(-4, 4)\n",
    "        dy = random.randint(-4, 4)\n",
    "        new_x = min(max(x + dx, 0), self.max_pos)\n",
    "        new_y = min(max(y + dy, 0), self.max_pos)\n",
    "\n",
    "        moved = torch.zeros_like(canvas)\n",
    "        moved[:, new_y:new_y+self.digit_size, new_x:new_x+self.digit_size] = img\n",
    "\n",
    "        return canvas.expand(3, -1, -1), moved.expand(3, -1, -1)  # Convert to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb90125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_dataloaders(data_root=\"./data\", batch_size=8):\n",
    "    full_train = MovingMNISTPairs(data_root, train=True)\n",
    "    val_size = int(0.1 * len(full_train))\n",
    "    train_size = len(full_train) - val_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c00932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CameraSequentialPairs(Dataset):\n",
    "    def __init__(self, root, transform=None, min_offset=1, max_offset=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory containing camera folders.\n",
    "            transform: Transformations to apply to each image.\n",
    "            min_offset (int): Minimum frame difference between pairs.\n",
    "            max_offset (int): Maximum frame difference between pairs.\n",
    "        \"\"\"\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.min_offset = min_offset\n",
    "        self.max_offset = max_offset\n",
    "        self.sequences = []\n",
    "\n",
    "        # List all camera directories\n",
    "        camera_dirs = sorted([os.path.join(root, d) for d in os.listdir(root)\n",
    "                              if os.path.isdir(os.path.join(root, d))])\n",
    "        \n",
    "        # For each camera directory, get sorted image paths (using numeric sorting)\n",
    "        for cam_dir in camera_dirs:\n",
    "            image_dir = os.path.join(cam_dir, \"images\")\n",
    "            if not os.path.exists(image_dir):\n",
    "                continue\n",
    "            image_files = sorted(\n",
    "                [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')],\n",
    "                key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[-1])\n",
    "            )\n",
    "            if len(image_files) > 1:\n",
    "                self.sequences.append(image_files)\n",
    "        \n",
    "        # Build cumulative lengths for indexing purposes\n",
    "        self.cumulative_lengths = []\n",
    "        total = 0\n",
    "        for seq in self.sequences:\n",
    "            # Each valid pair comes from a starting image (all except the last)\n",
    "            total += len(seq) - 1\n",
    "            self.cumulative_lengths.append(total)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_lengths[-1] if self.cumulative_lengths else 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    # Find which sequence to use\n",
    "        seq_idx = 0\n",
    "        while idx >= self.cumulative_lengths[seq_idx]:\n",
    "            seq_idx += 1\n",
    "        seq = self.sequences[seq_idx]\n",
    "        \n",
    "        local_idx = idx if seq_idx == 0 else idx - self.cumulative_lengths[seq_idx - 1]\n",
    "        \n",
    "        # Check that the index is valid\n",
    "        if local_idx >= len(seq) - 1:\n",
    "            raise IndexError(\"Local index out of range for the sequence\")\n",
    "        \n",
    "        img1_path = seq[local_idx]\n",
    "\n",
    "        # Compute the maximum valid offset\n",
    "        remaining = len(seq) - 1 - local_idx\n",
    "        effective_max_offset = min(self.max_offset, remaining)\n",
    "        \n",
    "        # In case effective_max_offset < min_offset, use effective_max_offset\n",
    "        # Otherwise, randomly sample\n",
    "        offset = effective_max_offset  # or if you want random:\n",
    "        # offset = random.randint(self.min_offset, effective_max_offset)\n",
    "\n",
    "        img2_idx = local_idx + offset\n",
    "        \n",
    "        img2_path = seq[img2_idx]\n",
    "\n",
    "        try:\n",
    "            img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "            img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images: {img1_path} or {img2_path}: {e}\")\n",
    "            # Optionally, raise an error here or return a default dummy tensor\n",
    "            raise e\n",
    "        \n",
    "        return self.transform(img1), self.transform(img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b08efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset pairs: 199998\n",
      "Validation dataset pairs: 19998\n",
      "Batch shapes: torch.Size([16, 3, 128, 128]) torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assuming CameraSequentialPairs is defined as in the previous code block.\n",
    "# For example:\n",
    "# class CameraSequentialPairs(Dataset):\n",
    "#     ... (code from previous message) ...\n",
    "\n",
    "def get_camera_dataloaders(data_root=\"./dataset_00\", batch_size=8, min_offset=5, max_offset=30, num_workers=8):\n",
    "    # For training, we use the \"train\" subfolder within the data root.\n",
    "    train_dataset = CameraSequentialPairs(\n",
    "        root=os.path.join(data_root, \"train\"),\n",
    "        transform=transforms.ToTensor(),\n",
    "        min_offset=min_offset,\n",
    "        max_offset=max_offset\n",
    "    )\n",
    "    \n",
    "    # Similarly, for validation, use the \"val\" subfolder.\n",
    "    val_dataset = CameraSequentialPairs(\n",
    "        root=os.path.join(data_root, \"val\"),\n",
    "        transform=transforms.ToTensor(),\n",
    "        min_offset=min_offset,\n",
    "        max_offset=max_offset\n",
    "    )\n",
    "    \n",
    "    # Optionally, print out dataset sizes for debugging\n",
    "    print(\"Training dataset pairs:\", len(train_dataset))\n",
    "    print(\"Validation dataset pairs:\", len(val_dataset))\n",
    "    \n",
    "    # Build DataLoaders with shuffling for training and no shuffle for validation.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Example usage:\n",
    "train_loader, val_loader = get_camera_dataloaders(\n",
    "    data_root=\"./dataset_00\", batch_size=16, min_offset=1, max_offset=5, num_workers=8\n",
    ")\n",
    "\n",
    "# For testing, iterate over one batch:\n",
    "for img1, img2 in train_loader:\n",
    "    print(\"Batch shapes:\", img1.shape, img2.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3440bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset pairs: 199998\n",
      "Validation dataset pairs: 19998\n",
      "12500\n",
      "torch.Size([16, 3, 128, 128]) torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_camera_dataloaders(data_root=\"./dataset_00\", batch_size=16)\n",
    "print(len(train_loader))\n",
    "for img1, img2 in train_loader:\n",
    "    print(img1.shape, img2.shape)  # e.g., [16, 3, H, W]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = L.Trainer(max_epochs=200, callbacks=[AlphaScheduler()])\n",
    "# model = LitKeypointDetector(KeypointPredictor(100), ImageEncoder(), ImageDecoder(64, 100))\n",
    "\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4fc3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=200, nhead=8, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "out = transformer_encoder(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78ce0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceKeypointDetector(nn.Module):\n",
    "    def __init__(self, keypoint_detector):\n",
    "        super().__init__()\n",
    "        self.keypoint_detector = keypoint_detector  # This is your existing 4D keypoint model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: input tensor of shape (B, T, C, H, W)\n",
    "        Returns:\n",
    "          Output: tensor of shape (B, T, num_keypoints, 2)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        # Flatten batch and sequence dimensions\n",
    "        x_flat = x.view(B * T, C, H, W)\n",
    "        # Process with keypoint detector (assumed to output (B*T, num_keypoints, 2))\n",
    "        keypoints_flat = self.keypoint_detector(x_flat)[0]  # For example, if keypoint_detector returns (grid_keypoints, soft_keypoints, heatmaps)\n",
    "        # Reshape the output back to (B, T, num_keypoints, 2)\n",
    "        keypoints = keypoints_flat.view(B, T, -1, 2)\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c3ca6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarikipati7/.env/ssta_env/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100, 200])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "seq_detector = SequenceKeypointDetector(KeypointPredictor(100))\n",
    "# Sample input: (64, 100, 3, 128, 128)\n",
    "input_tensor = torch.randn(8, 100, 3, 128, 128)\n",
    "output_keypoints = seq_detector(input_tensor)\n",
    "transformer_input = output_keypoints.view(8,100,-1)\n",
    "print(transformer_input.size())  # Should print torch.Size([64, 100, 100, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98984c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer_encoder(transformer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c123bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 100, 200])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPredictorT2NOD(L.LightningModule):\n",
    "    def __init__(self, keypoint_encoder, transformer_encoder, DecoderT2NOD):\n",
    "        super().__init__()\n",
    "        self.keypoint_encoder = keypoint_encoder\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        \n",
    "    def training_step(self, *args, **kwargs):\n",
    "        return super().training_step(*args, **kwargs)\n",
    "    def validation_step(self, *args, **kwargs):\n",
    "        return super().validation_step(*args, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
