{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f86a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149436e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarikipati7/.env/ssta_env/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "x = torch.rand(1,3,128,128)\n",
    "model = KeypointPredictor()\n",
    "out, soft, m = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079d8536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,64,32,32)\n",
    "y = torch.rand(1,100,2)\n",
    "\n",
    "model = ImageDecoder(64, 100, 3)\n",
    "out = model(x,y)\n",
    "out.shape\n",
    "\n",
    "model = ImageEncoder()\n",
    "out = model(torch.rand(1,3,128,128))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e03aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "class AlphaScheduler(Callback):\n",
    "    def __init__(self, warmup_epochs=10, ramp_epochs=20, final_alpha=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            warmup_epochs: Number of epochs to keep alpha = 0\n",
    "            ramp_epochs: Number of epochs to linearly ramp up alpha after warmup\n",
    "            final_alpha: Target alpha value\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.ramp_epochs = ramp_epochs\n",
    "        self.final_alpha = final_alpha\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "\n",
    "        if epoch < self.warmup_epochs:\n",
    "            alpha = 0.0\n",
    "        elif epoch < self.warmup_epochs + self.ramp_epochs:\n",
    "            # Linear ramp from 0 to final_alpha\n",
    "            ramp_progress = (epoch - self.warmup_epochs) / self.ramp_epochs\n",
    "            alpha = ramp_progress * self.final_alpha\n",
    "        else:\n",
    "            alpha = self.final_alpha\n",
    "\n",
    "        pl_module.alpha = alpha\n",
    "\n",
    "def draw_keypoints_on_image(image_tensor, keypoints, color='r'):\n",
    "    \"\"\"\n",
    "    image_tensor: [3, H, W] (float in [0,1])\n",
    "    keypoints: [N, 2] with (x, y) format in image coordinates\n",
    "    \"\"\"\n",
    "    image = TF.to_pil_image(image_tensor.cpu())\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    for x, y in keypoints.cpu():\n",
    "        plt.scatter(x, y, c=color, s=10)\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='jpeg', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    pil_image = Image.open(buf)\n",
    "    return TF.to_tensor(pil_image)\n",
    "    \n",
    "class LitKeypointDetector(L.LightningModule):\n",
    "    def __init__(self, keypoint_encoder, feature_encoder, feature_decoder):\n",
    "        super().__init__()\n",
    "        self.keypoint_generator = keypoint_encoder\n",
    "        self.feature_encoder = feature_encoder\n",
    "        self.image_reconstructor = feature_decoder\n",
    "        self.alpha = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        v1, vt = batch\n",
    "        \n",
    "        v1_features = self.feature_encoder(v1)\n",
    "\n",
    "        _, _, v1_heatmaps = self.keypoint_generator(v1)\n",
    "        _, vt_soft_keypoints, vt_heatmaps = self.keypoint_generator(vt)\n",
    "\n",
    "        vt_pred = self.image_reconstructor(v1_features, vt_soft_keypoints)\n",
    "        loss, mse, condens = self.keypoint_loss(vt, vt_pred, v1_heatmaps, vt_heatmaps)\n",
    "        self.log(\"train/total_loss\", loss)\n",
    "        self.log(\"train/mse\", mse)\n",
    "        self.log(\"train/condensation_loss\", condens)\n",
    "        self.log(\"alpha\", self.alpha)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        v1, vt = batch\n",
    "\n",
    "        # Encode features from v0\n",
    "        v1_features = self.feature_encoder(v1)\n",
    "\n",
    "        # Get keypoints from v0 and v1 separately\n",
    "        _, soft_kp_vt, v1_heatmaps = self.keypoint_generator(v1)\n",
    "        _, soft_kp_vt, vt_heatmaps = self.keypoint_generator(vt)\n",
    "\n",
    "        # Predict v1 using v0 features + v1 keypoints\n",
    "        vt_pred = self.image_reconstructor(v1_features, soft_kp_vt)\n",
    "        loss, mse, condens = self.keypoint_loss(vt, vt_pred, v1_heatmaps, vt_heatmaps)\n",
    "\n",
    "        self.log(\"val/total_loss\", loss)\n",
    "        self.log(\"val/mse\", mse)\n",
    "        self.log(\"val/condensation_loss\", condens)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            B, C, img_H, img_W = v1.shape\n",
    "            _, _, heatmap_H, heatmap_W = vt_heatmaps.shape\n",
    "\n",
    "            scale_x = img_W / heatmap_W\n",
    "            scale_y = img_H / heatmap_H\n",
    "\n",
    "            # Rescale keypoints\n",
    "            scaled_kps_v0 = soft_kp_vt[0].clone()\n",
    "            scaled_kps_v0[..., 0] *= scale_x\n",
    "            scaled_kps_v0[..., 1] *= scale_y\n",
    "\n",
    "            scaled_kps_v1 = soft_kp_vt[0].clone()\n",
    "            scaled_kps_v1[..., 0] *= scale_x\n",
    "            scaled_kps_v1[..., 1] *= scale_y\n",
    "\n",
    "            # Clamp images for visualization\n",
    "            v0_img = v1[0].clamp(0, 1)\n",
    "            v1_img = vt[0].clamp(0, 1)\n",
    "            v1_pred_img = vt_pred[0].clamp(0, 1)\n",
    "\n",
    "            # Overlay keypoints\n",
    "            vis_v0 = draw_keypoints_on_image(v0_img, scaled_kps_v0, color='b')  # Blue keypoints\n",
    "            vis_v1 = draw_keypoints_on_image(v1_img, scaled_kps_v1, color='r')  # Red keypoints\n",
    "            vis_pred = draw_keypoints_on_image(v1_pred_img, scaled_kps_v1, color='r')\n",
    "\n",
    "            # Combine visuals\n",
    "            grid = make_grid([vis_v0, vis_v1, vis_pred])\n",
    "            self.logger.experiment.add_image(\"val/visualize_key_reconstruction\", grid, self.current_epoch)\n",
    "\n",
    "            # Log heatmaps of v1\n",
    "            sample_heatmaps = vt_heatmaps[0].unsqueeze(1)  # [K, 1, H, W]\n",
    "            heatmap_grid = make_grid(sample_heatmaps, nrow=10, normalize=True, scale_each=True)\n",
    "            self.logger.experiment.add_image(\"val/heatmaps_v1\", heatmap_grid, self.current_epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def keypoint_loss(self, vt, vt_pred, v0_heat, vt_heat):\n",
    "\n",
    "        mse = nn.functional.mse_loss(vt, vt_pred)\n",
    "\n",
    "\n",
    "        # condensation_loss_1 = (v0_heat.amax(dim=(-1,-2)) - v0_heat.mean(dim=(-1,-2))).mean()\n",
    "        # condensation_loss_t = (vt_heat.amax(dim=(-1,-2)) - vt_heat.mean(dim=(-1,-2))).mean()\n",
    "        # condensation_loss = -(condensation_loss_1 + condensation_loss_t)/2\n",
    "        condensation_loss_1 = self.condensation_loss_entropy(v0_heat)\n",
    "        condensation_loss_t = self.condensation_loss_entropy(vt_heat)\n",
    "        condensation_loss = (condensation_loss_1 + condensation_loss_t)/2\n",
    "\n",
    "        return mse+self.alpha*condensation_loss, mse, condensation_loss\n",
    "    \n",
    "    def condensation_loss_entropy(self, heatmaps):\n",
    "        # heatmaps: [B, K, H, W] (softmaxed)\n",
    "        eps = 1e-8\n",
    "        log_h = torch.log(heatmaps + eps)\n",
    "        entropy = -torch.sum(heatmaps * log_h, dim=(-2, -1))  # [B, K]\n",
    "        return entropy.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c00932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CameraSequentialPairs(Dataset):\n",
    "    def __init__(self, root, transform=None, min_offset=1, max_offset=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory containing camera folders.\n",
    "            transform: Transformations to apply to each image.\n",
    "            min_offset (int): Minimum frame difference between pairs.\n",
    "            max_offset (int): Maximum frame difference between pairs.\n",
    "        \"\"\"\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.min_offset = min_offset\n",
    "        self.max_offset = max_offset\n",
    "        self.sequences = []\n",
    "\n",
    "        # List all camera directories\n",
    "        camera_dirs = sorted([os.path.join(root, d) for d in os.listdir(root)\n",
    "                              if os.path.isdir(os.path.join(root, d))])\n",
    "        \n",
    "        # For each camera directory, get sorted image paths (using numeric sorting)\n",
    "        for cam_dir in camera_dirs:\n",
    "            image_dir = os.path.join(cam_dir, \"images\")\n",
    "            if not os.path.exists(image_dir):\n",
    "                continue\n",
    "            image_files = sorted(\n",
    "                [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')],\n",
    "                key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[-1])\n",
    "            )\n",
    "            if len(image_files) > 1:\n",
    "                self.sequences.append(image_files)\n",
    "        \n",
    "        # Build cumulative lengths for indexing purposes\n",
    "        self.cumulative_lengths = []\n",
    "        total = 0\n",
    "        for seq in self.sequences:\n",
    "            # Each valid pair comes from a starting image (all except the last)\n",
    "            total += len(seq) - 1\n",
    "            self.cumulative_lengths.append(total)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_lengths[-1] if self.cumulative_lengths else 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    # Find which sequence to use\n",
    "        seq_idx = 0\n",
    "        while idx >= self.cumulative_lengths[seq_idx]:\n",
    "            seq_idx += 1\n",
    "        seq = self.sequences[seq_idx]\n",
    "        \n",
    "        local_idx = idx if seq_idx == 0 else idx - self.cumulative_lengths[seq_idx - 1]\n",
    "        \n",
    "        # Check that the index is valid\n",
    "        if local_idx >= len(seq) - 1:\n",
    "            raise IndexError(\"Local index out of range for the sequence\")\n",
    "        \n",
    "        img1_path = seq[local_idx]\n",
    "\n",
    "        # Compute the maximum valid offset\n",
    "        remaining = len(seq) - 1 - local_idx\n",
    "        effective_max_offset = min(self.max_offset, remaining)\n",
    "        \n",
    "        # In case effective_max_offset < min_offset, use effective_max_offset\n",
    "        # Otherwise, randomly sample\n",
    "        offset = effective_max_offset  # or if you want random:\n",
    "        # offset = random.randint(self.min_offset, effective_max_offset)\n",
    "\n",
    "        img2_idx = local_idx + offset\n",
    "        \n",
    "        img2_path = seq[img2_idx]\n",
    "\n",
    "        try:\n",
    "            img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "            img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images: {img1_path} or {img2_path}: {e}\")\n",
    "            # Optionally, raise an error here or return a default dummy tensor\n",
    "            raise e\n",
    "        \n",
    "        return self.transform(img1), self.transform(img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b08efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset pairs: 199998\n",
      "Validation dataset pairs: 19998\n",
      "Batch shapes: torch.Size([16, 3, 128, 128]) torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assuming CameraSequentialPairs is defined as in the previous code block.\n",
    "# For example:\n",
    "# class CameraSequentialPairs(Dataset):\n",
    "#     ... (code from previous message) ...\n",
    "\n",
    "def get_camera_dataloaders(data_root=\"./dataset_00\", batch_size=8, min_offset=5, max_offset=30, num_workers=8):\n",
    "    # For training, we use the \"train\" subfolder within the data root.\n",
    "    train_dataset = CameraSequentialPairs(\n",
    "        root=os.path.join(data_root, \"train\"),\n",
    "        transform=transforms.ToTensor(),\n",
    "        min_offset=min_offset,\n",
    "        max_offset=max_offset\n",
    "    )\n",
    "    \n",
    "    # Similarly, for validation, use the \"val\" subfolder.\n",
    "    val_dataset = CameraSequentialPairs(\n",
    "        root=os.path.join(data_root, \"val\"),\n",
    "        transform=transforms.ToTensor(),\n",
    "        min_offset=min_offset,\n",
    "        max_offset=max_offset\n",
    "    )\n",
    "    \n",
    "    # Optionally, print out dataset sizes for debugging\n",
    "    print(\"Training dataset pairs:\", len(train_dataset))\n",
    "    print(\"Validation dataset pairs:\", len(val_dataset))\n",
    "    \n",
    "    # Build DataLoaders with shuffling for training and no shuffle for validation.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Example usage:\n",
    "train_loader, val_loader = get_camera_dataloaders(\n",
    "    data_root=\"./dataset_00\", batch_size=16, min_offset=1, max_offset=5, num_workers=8\n",
    ")\n",
    "\n",
    "# For testing, iterate over one batch:\n",
    "for img1, img2 in train_loader:\n",
    "    print(\"Batch shapes:\", img1.shape, img2.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3440bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset pairs: 199998\n",
      "Validation dataset pairs: 19998\n",
      "12500\n",
      "torch.Size([16, 3, 128, 128]) torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_camera_dataloaders(data_root=\"./dataset_00\", batch_size=16)\n",
    "print(len(train_loader))\n",
    "for img1, img2 in train_loader:\n",
    "    print(img1.shape, img2.shape)  # e.g., [16, 3, H, W]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = L.Trainer(max_epochs=200, callbacks=[AlphaScheduler()])\n",
    "# model = LitKeypointDetector(KeypointPredictor(100), ImageEncoder(), ImageDecoder(64, 100))\n",
    "\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4fc3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=200, nhead=8, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "# out = transformer_encoder(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ce0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceKeypointDetector(nn.Module):\n",
    "    def __init__(self, keypoint_detector):\n",
    "        super().__init__()\n",
    "        self.keypoint_detector = keypoint_detector  # This is your existing 4D keypoint model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: input tensor of shape (B, T, C, H, W)\n",
    "        Returns:\n",
    "          Output: tensor of shape (B, T, num_keypoints, 2)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        # Flatten batch and sequence dimensions\n",
    "        x_flat = x.view(B * T, C, H, W)\n",
    "        # Process with keypoint detector (assumed to output (B*T, num_keypoints, 2))\n",
    "        keypoints_flat = self.keypoint_detector(x_flat)[0]  # For example, if keypoint_detector returns (grid_keypoints, soft_keypoints, heatmaps)\n",
    "        # Reshape the output back to (B, T, num_keypoints, 2)\n",
    "        keypoints = keypoints_flat.view(B, T, -1, 2)\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3ca6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarikipati7/.env/ssta_env/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100, 200])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "seq_detector = SequenceKeypointDetector(KeypointPredictor(100))\n",
    "# Sample input: (64, 100, 3, 128, 128)\n",
    "input_tensor = torch.randn(8, 100, 3, 128, 128)\n",
    "output_keypoints = seq_detector(input_tensor)\n",
    "transformer_input = output_keypoints.view(8,100,-1)\n",
    "print(transformer_input.size())  # Should print torch.Size([64, 100, 100, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98984c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = transformer_encoder(transformer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c123bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a602a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses.masked_mse import masked_mse\n",
    "from losses.dice import dice_loss\n",
    "import torchvision.utils as vutils\n",
    "from utils import draw_keypoints_on_image\n",
    "\n",
    "class LitPredictorT2NOD(L.LightningModule):\n",
    "    def __init__(self, image_encoder, seq_keypoint_encoder, transformer_encoder, T2N_decoder, future_decoder, num_keypoints):\n",
    "        super().__init__()\n",
    "        self.seq_keypoint_encoder = seq_keypoint_encoder\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.image_decoder = T2N_decoder\n",
    "        D = 160  # flatten dim: T × (K × 2)\n",
    "        T = 50\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, T, D))  # [1, T, D]\n",
    "        \n",
    "        self.linear_projto_attnn = nn.Linear(num_keypoints*2, D)\n",
    "        self.transform_to_kps = nn.Sequential(\n",
    "            nn.Linear(D, D),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(D, num_keypoints * 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.seq_keypoint_encoder.eval()\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.future_predictor = future_decoder\n",
    "        for param in self.future_predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.sample_size = 10\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seq_images, t2nod, future_images= batch\n",
    "        with torch.no_grad():\n",
    "            seq_keypoints = self.seq_keypoint_encoder(seq_images)\n",
    "\n",
    "        B, T, num_kps, _ = seq_keypoints.shape\n",
    "        flat_seq_keypoints = (seq_keypoints/32.0).view(B, T, -1)\n",
    "        transformer_input = self.act(self.linear_projto_attnn(flat_seq_keypoints))\n",
    "        transformer_input = transformer_input + self.pos_embed[:, :T]\n",
    "        \n",
    "        flat_pred_kepoints = self.transformer_encoder(transformer_input)\n",
    "        flat_pred_kepoints = self.transform_to_kps(flat_pred_kepoints)\n",
    "        pred_keypoints = flat_pred_kepoints.view(B,T*num_kps,2)\n",
    "        pred_keypoints = 32*((pred_keypoints+1)/2)\n",
    "        last_frame = seq_images[:, -1, :, :, :]\n",
    "        features = self.image_encoder(last_frame) # B, C, H, W\n",
    "\n",
    "        features_repeated = features.unsqueeze(1).expand(-1, T, -1, -1, -1)\n",
    "\n",
    "        future_keypoints = pred_keypoints.view(B,T, num_kps, 2)\n",
    "\n",
    "        sampled_idxs = torch.randperm(T)[:self.sample_size]\n",
    "        pred_keypoints_sampled = future_keypoints[:, sampled_idxs]\n",
    "        future_features_sampled = features_repeated[:, sampled_idxs]\n",
    "\n",
    "        future_keypoints = pred_keypoints_sampled.view(B*self.sample_size, num_kps, 2)\n",
    "        features_repeated = future_features_sampled.reshape(B*self.sample_size, 64, 32, 32)\n",
    "        \n",
    "        future_images_pred = self.future_predictor(features_repeated, future_keypoints)\n",
    "        future_loss = F.mse_loss(future_images_pred.view(B*self.sample_size,-1), (future_images[:,sampled_idxs].view(B*self.sample_size,-1)*2)-1)\n",
    "        \n",
    "        out = self.image_decoder(features, pred_keypoints)\n",
    "        \n",
    "        total_loss, bce, t2no_mse, t2nd_mse, t2no_mask = masked_mse(out, t2nod)\n",
    "        dice = dice_loss(out[:,0], t2no_mask)\n",
    "        \n",
    "        self.log(\"train/masked_mse\", total_loss)\n",
    "        self.log(\"train/bce\", bce)\n",
    "        self.log(\"train/t2no_mse\", t2no_mse)\n",
    "        self.log(\"train/t2nd_mse\", t2nd_mse)\n",
    "        self.log(\"train/future_loss\", future_loss)\n",
    "        self.log(\"train/total_loss\", total_loss + future_loss)\n",
    "        self.log(\"train/dice_loss\", dice)\n",
    "\n",
    "        return future_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seq_images, t2nod, future_images = batch\n",
    "        with torch.no_grad():\n",
    "            seq_keypoints = self.seq_keypoint_encoder(seq_images)\n",
    "\n",
    "            B, T, num_kps, _ = seq_keypoints.shape\n",
    "            flat_seq_keypoints = (seq_keypoints/32.0).view(B, T, -1)\n",
    "            transformer_input = self.act(self.linear_projto_attnn(flat_seq_keypoints))\n",
    "            transformer_input = transformer_input + self.pos_embed[:, :T]\n",
    "\n",
    "\n",
    "            flat_pred_kepoints = self.transformer_encoder(transformer_input)\n",
    "            flat_pred_kepoints = self.transform_to_kps(flat_pred_kepoints)\n",
    "            pred_keypoints = flat_pred_kepoints.view(B,T*num_kps,2)\n",
    "            pred_keypoints = 32*((pred_keypoints+1)/2)\n",
    "            last_frame = seq_images[:, -1, :, :, :]\n",
    "            features = self.image_encoder(last_frame) # B, C, H, W\n",
    "            \n",
    "            features_repeated = features.unsqueeze(1).expand(-1, T, -1, -1, -1).reshape(B*T, 64, 32, 32)\n",
    "\n",
    "            future_keypoints = pred_keypoints.view(B*T, num_kps, 2)\n",
    "\n",
    "            future_images_pred = self.future_predictor(features_repeated, future_keypoints)\n",
    "            future_loss = F.mse_loss(future_images_pred.view(B*T,-1), (future_images.view(B*T,-1)*2)-1)\n",
    "\n",
    "            out, pred_heatmaps = self.image_decoder(features, pred_keypoints, return_heatmaps=True)\n",
    "            \n",
    "            total_loss, bce, t2no_mse, t2nd_mse, t2no_mask = masked_mse(out, t2nod)\n",
    "            dice = dice_loss(out[:,0], t2no_mask)\n",
    "        \n",
    "            self.log(\"val/masked_mse\", total_loss)\n",
    "            self.log(\"val/bce\", bce)\n",
    "            self.log(\"val/t2no_mse\", t2no_mse)\n",
    "            self.log(\"val/t2nd_mse\", t2nd_mse)\n",
    "            self.log(\"val/future_loss\", future_loss)\n",
    "            self.log(\"val/dice_loss\", dice)\n",
    "            self.log(\"val/total_loss\", total_loss+future_loss)\n",
    "\n",
    "        if batch_idx == 0:  # Log only the first batch per epoch\n",
    "            last_image = seq_images[0,-1]\n",
    "            last_future_image = future_images[0,-1]\n",
    "            last_future_image_pred = future_images_pred.view(B,T,3,128,128)[0,-1]\n",
    "            scaled_last_keypoints = seq_keypoints[0,-1].clone()\n",
    "            scaled_last_keypoints[...,0] *= 128/32\n",
    "            scaled_last_keypoints[...,1] *= 128/32\n",
    "\n",
    "            scaled_pred_keypoints = pred_keypoints[0]\n",
    "            scaled_pred_keypoints[...,0] *= 128/32\n",
    "            scaled_pred_keypoints[...,1] *= 128/32\n",
    "\n",
    "            vis_seq_keypoints = draw_keypoints_on_image(last_image, scaled_last_keypoints, color = \"g\")\n",
    "            vis_pred_keypoints = draw_keypoints_on_image(last_image, scaled_pred_keypoints, color = \"b\")\n",
    "            vis_pred_keypoints_resized = F.interpolate(\n",
    "            vis_pred_keypoints.unsqueeze(0), size=vis_seq_keypoints.shape[-2:], mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "            \n",
    "            vis_future_keypoints = draw_keypoints_on_image(last_future_image, scaled_pred_keypoints[-100:,:])\n",
    "            vis_future_pred_keypoints = draw_keypoints_on_image((last_future_image_pred+1)/1, scaled_pred_keypoints[-100:,:])\n",
    "            \n",
    "            vis_future_keypoints_resized = F.interpolate(\n",
    "            vis_future_keypoints.unsqueeze(0), size=vis_seq_keypoints.shape[-2:], mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "            vis_future_pred_keypoints_resized = F.interpolate(\n",
    "            vis_future_pred_keypoints.unsqueeze(0), size=vis_seq_keypoints.shape[-2:], mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "            grid = make_grid([vis_seq_keypoints, vis_future_keypoints_resized, vis_future_pred_keypoints_resized, vis_pred_keypoints_resized])\n",
    "\n",
    "            self.logger.experiment.add_image(\"val/visualize_keypoints\", grid, self.current_epoch)\n",
    "\n",
    "\n",
    "            grid = self.visualize_predictions_for_tensorboard(\n",
    "                last_frame[0],      # RGB image\n",
    "                t2nod[0],           # [2, H, W] GT\n",
    "                out[0],             # [3, H, W] prediction\n",
    "                t2no_mask,             # [1, H, W]\n",
    "            )\n",
    "            num_kp_to_show = min(16, pred_heatmaps.shape[1])\n",
    "            heatmaps = pred_heatmaps[0, :num_kp_to_show]  # [K, H, W]\n",
    "            heatmaps = (heatmaps - heatmaps.min()) / (heatmaps.max() - heatmaps.min() + 1e-6)\n",
    "\n",
    "            # Optional: convert to RGB for visibility\n",
    "            heatmaps_rgb = heatmaps.unsqueeze(1).repeat(1, 3, 1, 1)  # [K, 3, H, W]\n",
    "            heatmap_grid = vutils.make_grid(heatmaps_rgb, nrow=4, padding=2)\n",
    "\n",
    "            self.logger.experiment.add_image(\"val/pred_heatmaps\", heatmap_grid, self.current_epoch)\n",
    "\n",
    "            self.logger.experiment.add_image(\"val/sample_grid\", grid, self.current_epoch)\n",
    "\n",
    "    def visualize_predictions_for_tensorboard(self, rgb_img, gt, pred, mask):\n",
    "        \"\"\"\n",
    "        Returns a grid of shape [3, H, 7*W] for logging as an image.\n",
    "        \"\"\"\n",
    "        def to_3ch(x):\n",
    "            if x.ndim == 2:\n",
    "                return x.unsqueeze(0).repeat(3, 1, 1)\n",
    "            elif x.shape[0] == 1:\n",
    "                return x.repeat(3, 1, 1)\n",
    "            return x\n",
    "\n",
    "        def normalize(x):\n",
    "            return (x - x.min()) / (x.max() - x.min() + 1e-5)\n",
    "\n",
    "        rgb_img = normalize(rgb_img)\n",
    "\n",
    "        t2no_mask     = (gt[0] != 1).float()\n",
    "        gt_t2no       = (gt[0]+1)/2\n",
    "        gt_t2nd       = (gt[1]+1)/2 + gt_t2no\n",
    "\n",
    "        pred_mask_asis = normalize(torch.sigmoid(pred[0]))\n",
    "        pred_mask = (torch.sigmoid(pred[0]) > 0.5).float()\n",
    "\n",
    "        pred_t2no_gt_mask     = ((pred[1]+1)/2) * mask[0] + (~mask[0])\n",
    "        pred_t2nd_gt_mask     = ((pred[2]+1)/2) * mask[0] + (~mask[0])\n",
    "\n",
    "\n",
    "        tiles = [\n",
    "            to_3ch(rgb_img),\n",
    "            to_3ch(t2no_mask),\n",
    "            to_3ch(normalize(gt_t2no)),\n",
    "            to_3ch(normalize(gt_t2nd)),\n",
    "            to_3ch(pred_mask),\n",
    "            to_3ch(pred_mask_asis),\n",
    "            to_3ch(normalize(pred_t2no_gt_mask)),\n",
    "            to_3ch(normalize(pred_t2nd_gt_mask)),\n",
    "            to_3ch(normalize(pred[1])),\n",
    "            to_3ch(normalize(pred[2])),\n",
    "        ]\n",
    "\n",
    "        grid = vutils.make_grid(tiles, nrow=7)\n",
    "        return grid\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d45e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def logsumexp_pooling(heatmaps, group_size=100, beta=10.0):\n",
    "    \"\"\"\n",
    "    Consolidate heatmaps using LogSumExp pooling.\n",
    "\n",
    "    Args:\n",
    "        heatmaps: Tensor of shape (B, K, H, W) where K = num_keypoints\n",
    "        num_groups: Number of final heatmaps desired\n",
    "        beta: Temperature parameter for LogSumExp\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (B, num_groups, H, W)\n",
    "    \"\"\"\n",
    "    B, K, H, W = heatmaps.shape\n",
    "\n",
    "    # Reshape: (B, num_groups, group_size, H, W)\n",
    "    grouped = heatmaps.view(B, K//group_size, group_size, H, W)\n",
    "\n",
    "    # LogSumExp pooling over the group dimension\n",
    "    pooled = (1.0 / beta) * torch.logsumexp(beta * grouped, dim=2)  # shape: (B, num_groups, H, W)\n",
    "\n",
    "    return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fb97d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have 10,000 keypoint heatmaps of size 32x32\n",
    "keypoints = torch.randn(8, 20000)  # Batch of 8\n",
    "keypoints = keypoints.view(8,10000,-1)\n",
    "features = torch.randn(8, 32, 32, 32)  # Batch of 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d189a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 128, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decoder = ImageDecoder(32, 1000, 3, condense=True)\n",
    "decoder(features, keypoints).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6190f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=200, nhead=8, batch_first=True)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "    \n",
    "# trainer = L.Trainer(max_epochs=200, callbacks=[AlphaScheduler()])\n",
    "# model = LitPredictorT2NOD(ImageEncoder(), SequenceKeypointDetector(KeypointPredictor(100)), transformer_encoder, ImageDecoder(64, 1000, 3, condense=True))\n",
    "\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13eef117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: torch.Size([20, 3, 128, 128])\n",
      "Target shape: torch.Size([2, 128, 128])\n",
      "Target shape: torch.Size([20, 3, 128, 128])\n",
      "Target shape: tensor(1.)\n",
      "Target shape: tensor(-1.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import re\n",
    "\n",
    "def natural_sort(l):\n",
    "    return sorted(l, key=lambda s: [int(text) if text.isdigit() else text.lower()\n",
    "                                    for text in re.split(r'(\\d+)', s)])\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 root_dir, \n",
    "                 mode='train', \n",
    "                 sequence_length=5,  \n",
    "                 input_transform=None, \n",
    "                 target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset folder (e.g., 'dataset_00').\n",
    "            mode (str): 'train' or 'val'.\n",
    "            sequence_length (int): Number of consecutive images in the input sequence.\n",
    "            input_transform (callable, optional): Transformation to be applied to each input image.\n",
    "            target_transform (callable, optional): Transformation to be applied to each target image.\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.samples = []  # Will hold dictionaries with sample info.\n",
    "        mode_dir = os.path.join(root_dir, mode)\n",
    "        \n",
    "        # Get each camera directory (e.g., camera_0, camera_1, etc.)\n",
    "        camera_dirs = sorted([d for d in os.listdir(mode_dir) if os.path.isdir(os.path.join(mode_dir, d))])\n",
    "        \n",
    "        for camera in camera_dirs:\n",
    "            camera_path = os.path.join(mode_dir, camera)\n",
    "            images_dir = os.path.join(camera_path, 'images')\n",
    "            t2no_dir   = os.path.join(camera_path, 't2no')\n",
    "            t2nd_dir   = os.path.join(camera_path, 't2nd')\n",
    "            \n",
    "            # List and sort file names in each folder\n",
    "            image_files = natural_sort(os.listdir(images_dir))\n",
    "            t2no_files  = natural_sort(os.listdir(t2no_dir))\n",
    "            t2nd_files  = natural_sort(os.listdir(t2nd_dir))\n",
    "            \n",
    "            # Check that all three folders contain the same number of files.\n",
    "            if not (len(image_files) == len(t2no_files) == len(t2nd_files)):\n",
    "                raise ValueError(f\"Mismatch in file counts in camera folder: {camera}\")\n",
    "            \n",
    "            num_frames = len(image_files)\n",
    "            # Only use valid starting indices where the full sequence exists.\n",
    "            for start_idx in range(num_frames - 2 * sequence_length + 1):\n",
    "                self.samples.append({\n",
    "                    \"images_dir\": images_dir,\n",
    "                    \"t2no_dir\": t2no_dir,\n",
    "                    \"t2nd_dir\": t2nd_dir,\n",
    "                    \"start_idx\": start_idx,\n",
    "                    \"file_list\": image_files  # Assuming the same ordering applies for all directories.\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.samples[idx]\n",
    "        images_dir = sample_info[\"images_dir\"]\n",
    "        t2no_dir   = sample_info[\"t2no_dir\"]\n",
    "        t2nd_dir   = sample_info[\"t2nd_dir\"]\n",
    "        start_idx  = sample_info[\"start_idx\"]\n",
    "        file_list  = sample_info[\"file_list\"]\n",
    "        \n",
    "        # Load the input sequence as RGB images.\n",
    "        input_sequence = []\n",
    "        future_sequence = []\n",
    "        for i in range(start_idx, start_idx + self.sequence_length):\n",
    "            img_path = os.path.join(images_dir, file_list[i])\n",
    "            future_path = os.path.join(images_dir, file_list[i+self.sequence_length])\n",
    "            # Open the image in RGB mode to retain three channels.\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            future_img = Image.open(future_path).convert('RGB')\n",
    "\n",
    "            if self.input_transform:\n",
    "                img = self.input_transform(img)\n",
    "                future_img = self.input_transform(future_img)\n",
    "            else:\n",
    "                # Default: convert image to tensor (C x H x W) with values in [0, 1]\n",
    "                img = T.ToTensor()(img)\n",
    "                future_img = T.ToTensor()(future_img)\n",
    "\n",
    "\n",
    "            input_sequence.append(img)\n",
    "            future_sequence.append(future_img)\n",
    "        \n",
    "        # Stack into a tensor of shape [sequence_length, 3, H, W]\n",
    "        input_sequence = torch.stack(input_sequence, dim=0)\n",
    "        future_sequence = torch.stack(future_sequence,dim=0)\n",
    "        \n",
    "        # For the target, choose the frame corresponding to the last image in the sequence.\n",
    "        target_idx = start_idx + self.sequence_length - 1\n",
    "        t2no_path = os.path.join(t2no_dir, file_list[target_idx])\n",
    "        t2nd_path = os.path.join(t2nd_dir, file_list[target_idx])\n",
    "        \n",
    "        # Load target images in grayscale.\n",
    "        t2no = Image.open(t2no_path).convert('L')\n",
    "        t2nd = Image.open(t2nd_path).convert('L')\n",
    "        \n",
    "        # Resize the targets to 128x128 pixels.\n",
    "        resize_transform = T.Resize((128, 128))\n",
    "        t2no = resize_transform(t2no)\n",
    "        t2nd = resize_transform(t2nd)\n",
    "        \n",
    "        delta = torch.from_numpy(((np.array(t2nd)-np.array(t2no))/25)-1).float().unsqueeze(0)\n",
    "        t2no = torch.from_numpy((np.array(t2no)/25)-1).float().unsqueeze(0)\n",
    "        \n",
    "        # Concatenate the two target images along the channel dimension.\n",
    "        target = torch.cat([t2no, delta], dim=0)  # Expected shape: [2, 128, 128]\n",
    "        \n",
    "        return input_sequence, target, future_sequence\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Define transforms if desired.\n",
    "    input_transform = T.Compose([\n",
    "        # Example: You can add T.Resize, T.RandomCrop, etc.\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    target_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    lambda x: (x/25)-1\n",
    "])\n",
    "\n",
    "    \n",
    "    # Create dataset instances for training and validation.\n",
    "    dataset_root = 'dataset_01'  # Replace with your dataset path.\n",
    "    train_dataset = SequenceDataset(root_dir=dataset_root, \n",
    "                                    mode='train', \n",
    "                                    sequence_length=20, \n",
    "                                    input_transform=input_transform, \n",
    "                                    target_transform=target_transform)\n",
    "    \n",
    "    val_dataset = SequenceDataset(root_dir=dataset_root, \n",
    "                                  mode='val', \n",
    "                                  sequence_length=20, \n",
    "                                  input_transform=input_transform, \n",
    "                                  target_transform=target_transform)\n",
    "    \n",
    "    # Test by loading one sample.\n",
    "    sample_in, sample_target, sample_future = train_dataset[0]\n",
    "    print(\"Input sequence shape:\", sample_in.shape)      # Expected: [sequence_length, 3, H, W]\n",
    "    print(\"Target shape:\", sample_target.shape)            # Expected: [2, 128, 128]\n",
    "    print(\"Target shape:\", sample_future.shape)            # Expected: [2, 128, 128]\n",
    "\n",
    "print(\"Target shape:\", sample_target.max())            # Expected: [2, 128, 128]\n",
    "print(\"Target shape:\", sample_target.min())            # Expected: [2, 128, 128]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db17d46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_predict_keypoints import LitKeypointDetector\n",
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "model = torch.load(\"/home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_3/checkpoints/epoch=99-step=1250000.ckpt\")\n",
    "\n",
    "\n",
    "keypoint_generator_state_dict = {\n",
    "    k.replace(\"keypoint_generator.\", \"\"): v\n",
    "    for k, v in model[\"state_dict\"].items()\n",
    "    if k.startswith(\"keypoint_generator.\")\n",
    "}\n",
    "# print(keypoint_generator_state_dict.keys())\n",
    "\n",
    "key_gen = KeypointPredictor(100)\n",
    "key_gen.load_state_dict(keypoint_generator_state_dict)\n",
    "# key_gen(torch.randn(1, 3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3c9d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x_map', 'y_map', 'conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'deconv1.weight', 'deconv1.bias', 'deconv2.weight', 'deconv2.bias', 'deconv3.weight', 'deconv3.bias', 'conv4.weight', 'conv4.bias', 'conv5.weight', 'conv5.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_decoder_state_dict = {\n",
    "    k.replace(\"image_reconstructor.\", \"\"): v\n",
    "    for k, v in model[\"state_dict\"].items()\n",
    "    if k.startswith(\"image_reconstructor.\")\n",
    "}\n",
    "print(image_decoder_state_dict.keys())\n",
    "# print(model[\"state_dict\"].items())\n",
    "future_decoder = ImageDecoder(64,100,3)\n",
    "future_decoder.load_state_dict(image_decoder_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a53260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type                     | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | seq_keypoint_encoder | SequenceKeypointDetector | 164 K  | eval \n",
      "1 | transformer_encoder  | TransformerEncoder       | 1.6 M  | train\n",
      "2 | image_encoder        | ImageEncoder             | 157 K  | train\n",
      "3 | image_decoder        | ImageDecoder             | 190 K  | train\n",
      "4 | linear_projto_attnn  | Linear                   | 32.2 K | train\n",
      "5 | transform_to_kps     | Sequential               | 58.0 K | train\n",
      "6 | act                  | SiLU                     | 0      | train\n",
      "7 | future_predictor     | ImageDecoder             | 190 K  | train\n",
      "  | other params         | n/a                      | 8.0 K  | n/a  \n",
      "--------------------------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "190 K     Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.642     Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "23        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  96%|█████████▌| 11981/12488 [33:27<01:24,  5.97it/s, v_num=8]    "
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = \"dataset_01\"\n",
    "SEQ_LEN = 50\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 8\n",
    "MAX_EPOCHS = 200\n",
    "\n",
    "# ------------------------------\n",
    "# Transforms\n",
    "# ------------------------------\n",
    "input_transform = T.Compose([\n",
    "    T.Resize((128, 128)),  # Or whatever size your model expects\n",
    "    T.ToTensor(),\n",
    "])\n",
    "# ------------------------------\n",
    "# Dataset & DataLoaders\n",
    "# ------------------------------\n",
    "train_dataset = SequenceDataset(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    mode=\"train\",\n",
    "    sequence_length=SEQ_LEN,\n",
    "    input_transform=input_transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "val_dataset = SequenceDataset(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    mode=\"val\",\n",
    "    sequence_length=SEQ_LEN,\n",
    "    input_transform=input_transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ------------------------------\n",
    "# Model Initialization\n",
    "# ------------------------------\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=160, nhead=4, batch_first=True, activation=nn.GELU(approximate=\"tanh\"), dim_feedforward=512)\n",
    "\n",
    "from models.keypoints.KeypointPredictor import KeypointPredictor\n",
    "from models.keypoints.ImageEncoder import ImageEncoder\n",
    "from models.keypoints.ImageDecoder import ImageDecoder\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "\n",
    "detector = KeypointPredictor(100)\n",
    "detector.load_state_dict(keypoint_generator_state_dict)\n",
    "seq_keypoint_encoder = SequenceKeypointDetector(detector)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "T2N_decoder = ImageDecoder(64,100,3, condense=True)\n",
    "# future_decoder = ImageDecoder(64,100,3)\n",
    "model = LitPredictorT2NOD(\n",
    "    image_encoder=image_encoder,\n",
    "    seq_keypoint_encoder=seq_keypoint_encoder,\n",
    "    transformer_encoder=transformer_encoder,\n",
    "    T2N_decoder=T2N_decoder,\n",
    "    future_decoder=future_decoder,\n",
    "    num_keypoints=100,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    ")\n",
    "# ckpt=\"/home/sgarikipati7/packages/SSTA_2025/lightning_logs/version_11/checkpoints/epoch=6-step=174832.ckpt\"\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c29c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
